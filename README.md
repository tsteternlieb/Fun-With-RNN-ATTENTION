# Fun-With-RNN-ATTENTION
Playing around with LSTMS and attention in the non infinite data setting... AKA Geometric Priors > Transformers
